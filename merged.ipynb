{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asumansaree/All-HackerRank-C-Challanges/blob/main/merged.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "hxhoqqBcaycz",
        "outputId": "f5471496-d8ae-4fb1-d417-a1c0a17d1e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c9cf2ace8ec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# call the constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# reads data.json as text and loads func returns it as json object (in dict structure)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: WordNetLemmatizer() takes no arguments"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import json\n",
        "import pickle # for serialization\n",
        "import numpy as np\n",
        "import codecs\n",
        "import io\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer # for stem find\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "from distutils.command.clean import clean\n",
        "from unittest import result # for serialization\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer # for stem find\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "lemmatizer = WordNetLemmatizer(\"tr\") # call the constructor\n",
        "\n",
        "# reads data.json as text and loads func returns it as json object (in dict structure)\n",
        "intents = json.load(codecs.open('tag_pattern_response_wordGroups.json', 'r', 'utf-8-sig'))\n",
        "\n",
        "# create empty lists\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?', '!', '.', ',']\n",
        "\n",
        "# iterate over the intents\n",
        "# nested for loop is necessary for key's inside dict\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        word_list = nltk.word_tokenize(pattern) # tokenize splits the sentence into words\n",
        "        words.extend(word_list) # extend = take the content and append it to the list\n",
        "        documents.append((word_list, intent['tag']))\n",
        "        if intent['tag'] not in classes:\n",
        "          classes.append(intent['tag'])\n",
        "\n",
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
        "words = sorted(set(words)) # set eliminate the duplicates, sorted trans it backs to list\n",
        "\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "# to save them into a file\n",
        "pickle.dump(words, open('words_demo.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes_demo.pkl', 'wb'))\n",
        "\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "  bag = []\n",
        "  word_patterns = document[0]\n",
        "  word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "  for word in words:\n",
        "    bag.append(1) if word in word_patterns else bag.append(0)\n",
        "\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(document[1])] = 1\n",
        "  training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer =sgd, metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbotmodel_demo.h5', hist)\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raç"
      ],
      "metadata": {
        "id": "w0hIlpo8eJAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "kE99OZUAchhT",
        "outputId": "c9a7924f-4089-4c3d-af7d-a184a49f9f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GO! Bot is running!\n",
            "SES ÇOK YÜKSEK SES ÖLÇÜMÜ \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-39b6c685c47c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-39b6c685c47c>\u001b[0m in \u001b[0;36mget_response\u001b[0;34m(intents_list, intents_json)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintents_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintents_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintents_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mlist_of_intents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintents_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ints:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintents_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "with io.open('tag_pattern_response_wordGroups.json', 'r', encoding='utf-8-sig') as f:\n",
        "  intents = json.loads(f.read())\n",
        "\n",
        "#intents = json.loads(open('data.json').read())\n",
        "\n",
        "words = pickle.load(open('words_demo.pkl', 'rb'))\n",
        "classes = pickle.load(open('classes_demo.pkl', 'rb'))\n",
        "model = load_model('chatbotmodel_demo.h5')\n",
        "\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "def bag_of_words(sentence):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0] * len(words)\n",
        "    for w in sentence_words:\n",
        "        for i, word in enumerate(words):\n",
        "            if word == w:\n",
        "                bag[i] = 1\n",
        "    return np.array(bag)\n",
        "\n",
        "def predict_class(sentence):\n",
        "    bow = bag_of_words(sentence)\n",
        "    res = model.predict(np.array([bow]))[0]\n",
        "    ERROR_TRESHOLD = 0.05\n",
        "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_TRESHOLD]\n",
        "    \n",
        "    results.sort(key=lambda x:x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
        "    return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "    tag = intents_list[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    print(\"ints:\\n\", intents_list)\n",
        "    # print(\"prob:\\n\", prob)\n",
        "    # print(\"list_of_intents:\\n\", list_of_intents)\n",
        "    \n",
        "    # best_probability = intents_list[0]['probability']\n",
        "    # best_match_response = list_of_intents['responses']\n",
        "    # for i in list_of_intents:\n",
        "    #     if i['tag'] == tag:\n",
        "    #         #result = random.choice(i['responses'])\n",
        "    #         if intents_list[i]['probability'] > best_probability:\n",
        "    #           best_probability = intents_list[i]['probability']\n",
        "    #           best_match_response = i['responses']\n",
        "    # print(best_probability)\n",
        "    # return best_match_response\n",
        "    best_prob = intents_list[0]['probability']\n",
        "    best_i = 0\n",
        "    for i in range(len(intents_list)):\n",
        "      if intents_list[i]['probability'] > best_prob:\n",
        "          best_prob = intents_list[i]['probability']\n",
        "          best_i = i\n",
        "    best_matched_tag = intents_list[best_i]['intent']\n",
        "    for i in list_of_intents:\n",
        "        if i['tag'] == best_matched_tag:\n",
        "          return i['responses']\n",
        "    return -1\n",
        "\n",
        "print(\"GO! Bot is running!\")\n",
        "\n",
        "while True:\n",
        "    message = input(\"\")\n",
        "    ints = predict_class(message)\n",
        "    res = get_response(ints, intents)\n",
        "    print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hadhrOmG6poM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "merged.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsqRt93Szr4zNDDL5o0Il7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}